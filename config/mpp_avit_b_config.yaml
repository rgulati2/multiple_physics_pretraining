basic_config: &basic_config
  # Run settings
  log_to_wandb: !!bool False # Use wandb integration
  log_to_screen: !!bool True # Log progress to screen.
  save_checkpoint: !!bool True # Save checkpoints
  checkpoint_save_interval: 5 # Save every # epochs - also saves "best" according to val loss
  debug_grad: !!bool True # Compute gradient/step_sizes/ect for debugging
  true_time: !!bool False # Debugging setting - sets num workers to zero and activates syncs
  num_data_workers: 1 # Generally pulling 8 cpu per process, so using 6 for DL - not sure if best ratio
  enable_amp: !!bool False # Use automatic mixed precision - blows up with low variance fields right now
  compile: !!bool False # Compile model - Does not currently work
  gradient_checkpointing: !!bool False # Whether to use gradient checkpointing - Slow, but lower memory
  exp_dir: '~/MPP' # Output path
  log_interval: 1 # How often to log - Don't think this is actually implemented
  pretrained: !!bool True # Whether to load a pretrained model
  #pretrained_ckpt_path: '~/MPP/basic_config/demo/training_checkpoints/MPP_AViT_B' #'~/MPP/basic_config/demo/training_checkpoints/ckpt.tar'
  #pretrained_ckpt_path: '/project/garikipa_1359/rahulgul/spatiotemporal/MPP1/multiple_physics_pretraining/~/MPP/basic_config/caseTest3a/training_checkpoints/ckpt.tar'
  #pretrained_ckpt_path: '/project/garikipa_1359/rahulgul/spatiotemporal/MPP1/multiple_physics_pretraining/~/MPP/basic_config/caseTest4/training_checkpoints/ckpt.tar_epoch10'
  #pretrained_ckpt_path: '/project/garikipa_1359/rahulgul/spatiotemporal/elasticity/test14a/multiple_physics_pretraining/~/MPP/basic_config/el_test14a/training_checkpoints/best_ckpt.tar'
  pretrained_ckpt_path: '/project/garikipa_1359/rahulgul/spatiotemporalMay25/heat/test1f_ana2/~/MPP/basic_config/forecast_1f1/training_checkpoints/ckpt.tar_epoch100'
  freeze_middle: !!bool False # Whether to freeze the middle layers of the model
  freeze_processor: !!bool False
  save_plot_dir: '/project/garikipa_1359/rahulgul/spatiotemporal/MPP1/multiple_physics_pretraining/plots_h_3b_1e-5/' 
  # wandb settings
  project: 'project' 
  group: 'debugging'
  entity: 'entity'
  # Training settings
  drop_path: 0.1
  batch_size: 1
  max_epochs: 10
  scheduler_epochs: -1
  epoch_size: 2000 # Artificial epoch size, more like batch size
  rescale_gradients: !!bool False # Activate hook that scales block gradients to norm 1
  optimizer: 'adan' # adam, adan, whatever else i end up adding - adan did better on HP sweep
  scheduler: 'cosine' # Only cosine implemented
  warmup_steps: 1000 # Warmup when not using DAdapt
  learning_rate: -1 # -1 means use DAdapt
  weight_decay: 1e-3 
  n_states: 12   # Number of state variables across the datasets - Can be larger than real number and things will just go unused
  state_names: ['Pressure', 'Vx', 'Vy', 'Density',  'Vx', 'Vy', 'Density', 'Pressure', 'uh'] # Should be sorted uh = Temperature state_variable
  dt: 1 # Striding of data - Not currently implemented > 1
  n_steps: 64 # Length of history to include in input
  enforce_max_steps: !!bool False # If false and n_steps > dataset steps, use dataset steps. Otherwise, raise Exception.
  accum_grad: 5  # Real batch size is accum * batch_size, real steps/"epoch" is epoch_size / accum
  # Model settings
  model_type: 'avit' # Only option so far
  block_type: 'axial' # Which type of block to use - if axial, next two fields must be set to define axial ops
  time_type: 'attention' # Conditional on block type
  space_type: 'axial_attention' # Conditional on block type
  tie_fields: !!bool False # Whether to use 1 embedding per field per data
  embed_dim: 768 # Dimension of internal representation - 192/384/768/1024 for Ti/S/B/L
  num_heads: 12 # Number of heads for attention - 3/6/12/16 for Ti/S/B/L
  processor_blocks: 12 # Number of transformer blocks in the backbone - 12/12/12/24 for Ti/S/B/L
  patch_size: [16, 16] # Actually currently hardcoded at 16
  bias_type: 'rel'  # Options rel, continuous, none
  # Data settings
  train_val_test: [.8, .1, .1]
  augmentation: !!bool False # Augmentation not implemented
  use_all_fields: !!bool True # Prepopulate the field metadata dictionary from dictionary in datasets
  tie_batches: !!bool False # Force everything in batch to come from one dset
  extended_names: !!bool False # Whether to use extended names - not currently implemented
  embedding_offset: 0  # Use when adding extra finetuning fields
  train_data_paths: [
    ['/project/garikipa_1359/rahulgul/spatiotemporal/MPP1/multiple_physics_pretraining/~/may2025/heat_ana_f_1e-5', 'heat', ''], 
   #['/project/garikipa_1359/rahulgul/spatiotemporal/MPP1/multiple_physics_pretraining/~/elasticity/elasticity1000_shear_bending', 'elasticity', ''],  
   #['/project/garikipa_1359/rahulgul/spatiotemporal/MPP1/multiple_physics_pretraining/~/elasticity/elasticity2000_shear_bending_dbc', 'elasticity', ''],  
  #['/project/garikipa_1359/rahulgul/spatiotemporal/MPP1/multiple_physics_pretraining/~/heatNew/heat1000', 'heat', ''], 
 #['~/PDEBench/2D', 'diffre2d', ''],
 #['~/CPG1', 'heat', ''],
 #['/project/garikipa_1359/rahulgul/spatiotemporal/MPP1/multiple_physics_pretraining/~/heat1000_t0-1', 'heat', ''],
 #['~/test2_DR', 'diffre2d', ''],
 #['~/PDEBench/smallDR', 'diffre2d', ''],
              ]
  valid_data_paths: [
     ['/project/garikipa_1359/rahulgul/spatiotemporal/MPP1/multiple_physics_pretraining/~/may2025/heat_ana_f_1e-5', 'heat', ''], 
    #['/project/garikipa_1359/rahulgul/spatiotemporal/MPP1/multiple_physics_pretraining/~/elasticity/elasticity1000_shear_bending', 'elasticity', ''],  
   #['/project/garikipa_1359/rahulgul/spatiotemporal/MPP1/multiple_physics_pretraining/~/elasticity/elasticity2000_shear_bending_dbc', 'elasticity', ''],  
  #['~/PDEBench/2D', 'diffre2d', ''],
  #['~/CPG1', 'heat', ''],
 #['/project/garikipa_1359/rahulgul/spatiotemporal/MPP1/multiple_physics_pretraining/~/heatNew/heat1000', 'heat', ''],
  #['~/test2_DR', 'diffre2d', ''],
              ]
  append_datasets: [] # List of datasets to append to the input/output projections for finetuning

finetune: &finetune
  <<: *basic_config
  max_epochs: 10
  train_val_test: [.8, .1, .1]
  accum_grad: 1
  pretrained: !!bool True
  group: 'debugging'
  pretrained_ckpt_path: '/project/garikipa_1359/rahulgul/spatiotemporal/heatNew/test11b/multiple_physics_pretraining/~/MPP/finetune/h_test11b/training_checkpoints/best_ckpt.tar'
  train_data_paths: [
    ['/project/garikipa_1359/rahulgul/spatiotemporal/MPP1/multiple_physics_pretraining/~/currated/h_a', 'heat', ''],
    #['/project/garikipa_1359/rahulgul/spatiotemporal/MPP1/multiple_physics_pretraining/~/elasticity/elasticity1000_shear_bending', 'elasticity', ''],  
   #['/project/garikipa_1359/rahulgul/spatiotemporal/MPP1/multiple_physics_pretraining/~/elasticity/elasticity2000_shear_bending_dbc', 'elasticity', ''],  
  #['/project/garikipa_1359/rahulgul/spatiotemporal/MPP1/multiple_physics_pretraining/~/heatNew/heat1000', 'heat', ''],
    #['~/CPG1', 'heat', ''],
    #['~/Test4_100_Heat', 'heat', ''],
    #['/PDEBench/2D/CFD/2D_Train_Turb', 'compNS', 'M1.0'],
	  #['~/PDEBench/2D', 'diffre2d', ''],
              ]
  valid_data_paths: [  # These are the same for all configs - uses split according to train_val_test
  ['/project/garikipa_1359/rahulgul/spatiotemporal/MPP1/multiple_physics_pretraining/~/currated/h_a', 'heat', ''],
  #['/project/garikipa_1359/rahulgul/spatiotemporal/MPP1/multiple_physics_pretraining/~/elasticity/elasticity1000_shear_bending', 'elasticity', ''],  
   #['/project/garikipa_1359/rahulgul/spatiotemporal/MPP1/multiple_physics_pretraining/~/elasticity/elasticity2000_shear_bending_dbc', 'elasticity', ''],  
  #['/project/garikipa_1359/rahulgul/spatiotemporal/MPP1/multiple_physics_pretraining/~/heatNew/heat1000', 'heat', ''],
  #['~/CPG1', 'heat', ''],
  #['~/Test4_100_Heat', 'heat', ''],
  #['~/PDEBench/2D', 'diffre2d', ''],
  #['/PDEBench/2D/CFD/2D_Train_Turb', 'compNS', 'M1.0'],
              ]
  embedding_offset: 10 # Number of fields in original model - FT fields start after this
  freeze_middle: !!bool False # Whether to freeze the middle layers of the model
  freeze_processor: !!bool False
  append_datasets: ['heat'] # List of datasets to append to the input/output projections for finetuning
  

frozen: &frozen
  <<: *finetune
  freeze_middle: !!bool True # Whether to freeze the middle layers of the model
  freeze_processor: !!bool False

less_frozen: &less_frozen
  <<: *finetune
  freeze_middle: !!bool True # Whether to freeze the middle layers of the model
  freeze_processor: !!bool True
